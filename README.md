
```markdown
# ğŸ¤– AGLLMTest (Hardware Metrics LLM)

**AGLLMTest**, yerel LLM'ler (Local Large Language Models) ile **Tool Calling (Function Calling)** yaklaÅŸÄ±mÄ±nÄ± Ã¶ÄŸrenmek, denemek ve uÃ§tan uca pratik etmek iÃ§in geliÅŸtirilmiÅŸ bir **Ã¶ÄŸrenme projesidir**.

Bu proje, sistem metriklerini (CPU, RAM, GPU) toplar, bir veritabanÄ±na yazar ve kullanÄ±cÄ±nÄ±n doÄŸal dil ile sorduÄŸu sorularÄ± (Ã¶rneÄŸin: *"Son 10 dakika CPU max nedir?"*) SQL sorgularÄ±na dÃ¶nÃ¼ÅŸtÃ¼rerek yanÄ±tlar.

---

## âš™ï¸ NasÄ±l Ã‡alÄ±ÅŸÄ±r?

Sistem ÅŸu 4 temel adÄ±mda iÅŸler:

1.  **Collector:** Belirli aralÄ±klarla CPU, RAM ve GPU metriklerini toplar.
2.  **Storage:** Toplanan metrikler **PostgreSQL** veritabanÄ±na yazÄ±lÄ±r.
3.  **LLM Orchestrator:** FastAPI Ã¼zerinden gelen doÄŸal dil sorularÄ± LLM tarafÄ±ndan yorumlanÄ±r.
4.  **Tool Execution:** LLM, soruyu cevaplamak iÃ§in uygun **SQL tool**'unu Ã§aÄŸÄ±rÄ±r ve elde ettiÄŸi veriyi yorumlayarak son kullanÄ±cÄ±ya cevap Ã¼retir.

> âš ï¸ **Ã–nemli Not:** Bu sÃ¼rÃ¼mde `hosts`, `host_id` veya `hostname` ayrÄ±mÄ± **yoktur**. Veriler tek bir metrik akÄ±ÅŸÄ± olarak kabul edilir. AynÄ± veritabanÄ±na birden fazla collector yazarsa veriler karÄ±ÅŸabilir.

---

## ğŸš€ Ã–zellikler

* **Collector Service:** `psutil` ve (varsa) `nvidia-smi` kullanarak veri toplar.
* **Database:** PostgreSQL 16.
* **API:** FastAPI tabanlÄ± REST API.
* **LLM Orchestrator:**
    * "Son 10 dk", "GeÃ§en 1 saat" gibi zaman ifadelerini ayrÄ±ÅŸtÄ±rÄ±r.
    * Uygun SQL fonksiyonunu seÃ§er.
    * Sorgu sonucunu LLM'e baÄŸlam olarak verip doÄŸal dil cevabÄ± Ã¼retir.
* **Otomatik Migration:** `docker-compose` iÃ§indeki `migrator` servisi, DB hazÄ±r olduÄŸunda otomatik olarak `alembic upgrade head` Ã§alÄ±ÅŸtÄ±rÄ±r.

---

## ğŸ“‹ Gereksinimler

### Ã‡alÄ±ÅŸtÄ±rmak Ä°Ã§in (Ã–nerilen)
* **Docker Desktop** (Windows/macOS) veya **Docker Engine** (Linux)
* **Docker Compose v2**

### Local LLM Ä°Ã§in
* OpenAI uyumlu endpoint sunan bir yerel LLM Ã§Ã¶zÃ¼mÃ¼.
* **Ã–neri:** [Ollama](https://ollama.com/)
* Proje `POST {LLM_BASE_URL}/chat/completions` adresine istek atar.

### GPU Metrikleri HakkÄ±nda
* Container iÃ§inde `nvidia-smi` eriÅŸimi yoksa GPU deÄŸerleri **random (rastgele)** Ã¼retilir.
* GerÃ§ek GPU metrikleri iÃ§in NVIDIA Driver + Container Runtime yapÄ±landÄ±rmasÄ± gereklidir.

---

## ğŸ“‚ Proje YapÄ±sÄ±

* `app/services/collector.py` â¤ Metrik toplayÄ±cÄ± servis.
* `app/api/v1/routers/llm.py` â¤ `/api/v1/llm/ask` endpoint'i.
* `app/llm/orchestrator.py` â¤ Tool Ã§aÄŸrÄ±larÄ± ve cevap Ã¼retim mantÄ±ÄŸÄ±.
* `app/llm/tools/specs/*.json` â¤ Tool ÅŸemalarÄ± (OpenAI formatÄ±).
* `app/llm/tools/sql/*.sql` â¤ Tool'larÄ±n Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ± SQL sorgularÄ±.
* `alembic/` â¤ VeritabanÄ± migration yÃ¶netimi.
* `docker-compose.yml` â¤ TÃ¼m servislerin (db, api, collector, migrator) orkestrasyonu.

---

## ğŸ”§ Kurulum ve YapÄ±landÄ±rma

### 1. Ortam DeÄŸiÅŸkenleri (.env)

Projeyi Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce `.env` dosyasÄ± oluÅŸturulmalÄ±dÄ±r. Ã–rnek dosyayÄ± kopyalayÄ±n:

```bash
cp .env.example .env

```

**Ã–rnek `.env` iÃ§eriÄŸi:**

```dotenv
# DB BaÄŸlantÄ±larÄ±
DATABASE_URL_ASYNC=postgresql+asyncpg://app:app@db:5432/hwdb
DATABASE_URL_SYNC=postgresql+psycopg://app:app@db:5432/hwdb

# LLM AyarlarÄ± (Ollama Ã–rneÄŸi)
# Docker iÃ§inden host makinedeki Ollama'ya eriÅŸim iÃ§in host.docker.internal kullanÄ±lÄ±r
LLM_BASE_URL=[http://host.docker.internal:11434/v1](http://host.docker.internal:11434/v1)
LLM_MODEL=llama3.1
LLM_TIMEOUT_SECONDS=60
LLM_MAX_TOOL_ITERATIONS=5

# Logging
LOG_LEVEL=INFO
LOG_DIR=/var/log/app

# Collector
METRICS_INTERVAL_SECONDS=10

```

### 2. BaÅŸlatma (Docker Compose)

Temiz bir baÅŸlangÄ±Ã§ yapmak (DB dahil her ÅŸeyi sÄ±fÄ±rdan kurmak) iÃ§in:

```bash
# Eski volume'leri temizle ve yeniden build et
docker compose down -v
docker compose up --build

```

Bu iÅŸlem sÄ±rasÄ±yla ÅŸunlarÄ± yapar:

1. Postgres volume silinir (veri sÄ±fÄ±rlanÄ±r).
2. Ä°majlar build edilir.
3. `migrator` servisi Ã§alÄ±ÅŸÄ±r ve tablolarÄ± oluÅŸturur.
4. Migration bitince `api` ve `collector` servisleri baÅŸlar.

### 3. Kontrol ve Loglar

Servislerin durumunu gÃ¶rmek iÃ§in:

```bash
docker compose ps

```

LoglarÄ± canlÄ± izlemek iÃ§in:

```bash
docker compose logs -f api      # API loglarÄ±
docker compose logs -f collector # Collector loglarÄ±
docker compose logs -f db       # VeritabanÄ± loglarÄ±

```

---

## ğŸ”Œ API KullanÄ±mÄ±

### Health Check

Sistemin ayakta olduÄŸunu doÄŸrulamak iÃ§in:

```bash
curl http://localhost:8000/api/v1/health
# Beklenen Cevap: {"status":"ok"}

```

### LLM ile Soru Sorma

Metriklerle ilgili soru sormak iÃ§in:

**Endpoint:** `POST http://localhost:8000/api/v1/llm/ask`

**Ã–rnek Ä°stek (Curl - Linux/Mac):**

```bash
curl -X POST http://localhost:8000/api/v1/llm/ask \
  -H "Content-Type: application/json" \
  -d '{"text":"Son 10 dk CPU max nedir?"}'

```

**Ã–rnek Ä°stek (PowerShell - Windows):**

```powershell
curl -X POST http://localhost:8000/api/v1/llm/ask `
  -H "Content-Type: application/json" `
  -d "{\"text\":\"Son 10 dk CPU max nedir?\"}"

```

**Beklenen Cevap:**

```json
{
  "answer": "Son 10 dakika iÃ§indeki maksimum CPU kullanÄ±mÄ± %45 olarak Ã¶lÃ§Ã¼lmÃ¼ÅŸtÃ¼r."
}

```

---

## ğŸ§  LLM ve Prompt KÄ±lavuzu

Sistem aÅŸaÄŸÄ±daki soru tiplerine ve zaman ifadelerine duyarlÄ±dÄ±r:

### Desteklenen Soru Tipleri

* **Zaman AralÄ±ÄŸÄ±:** "Son 10 dk CPU max nedir?", "GeÃ§en 1 saat GPU max kaÃ§?"
* **AnlÄ±k Durum:** "Åu an CPU kullanÄ±mÄ± kaÃ§?", "En gÃ¼ncel metrikleri gÃ¶ster." (Sistem "ÅŸu an" ifadesini pratikte son 5 dakika veya son snapshot olarak yorumlar).
* **Birimler:**
* Dakika: `dk`, `dakika`
* Saat: `saat`
* GÃ¼n: `gÃ¼n`


* **SayÄ± Ä°fadeleri:** "Son bir saat", "son on dakika" gibi TÃ¼rkÃ§e ifadeler desteklenir.

### Mevcut Tool'lar

LLM arka planda ÅŸu fonksiyonlarÄ± Ã§aÄŸÄ±rabilir:

* `get_latest_snapshot`
* `get_max_cpu_usage(minutes)`
* `get_max_cpu_temp(minutes)`
* `get_max_ram_usage_percent(minutes)`
* `get_max_gpu_utilization(minutes)`

---

## ğŸ¦™ Ollama Kurulumu (Local LLM)

Bu proje OpenAI uyumlu bir endpoint bekler. Ollama'yÄ± yerel LLM sunucusu olarak kullanmak iÃ§in adÄ±mlar:

1. **Kurulum:** [Ollama.com](https://ollama.com) Ã¼zerinden indirip kurun.
```bash
ollama --version

```


2. **Model Ä°ndirme:** Projede kullanacaÄŸÄ±nÄ±z modeli Ã§ekin (`.env` dosyasÄ±ndaki `LLM_MODEL` ile aynÄ± olmalÄ±dÄ±r).
```bash
ollama pull llama3.1
# veya
ollama pull mistral

```


3. **Ã‡alÄ±ÅŸtÄ±rma:**
```bash
ollama serve
# Default port: 11434

```


4. **Test:**
```bash
ollama run llama3.1 "Merhaba!"

```



---

## ğŸ›  Troubleshooting (Sorun Giderme)

| Hata / Durum | Ã‡Ã¶zÃ¼m |
| --- | --- |
| **`alembic is not recognized`** | Bu normaldir. Migration container iÃ§inde otomatik Ã§alÄ±ÅŸÄ±r. Elle Ã§alÄ±ÅŸtÄ±rmak iÃ§in: `docker compose exec api alembic upgrade head` |
| **`relation metrics_cpu does not exist`** | Migration Ã§alÄ±ÅŸmamÄ±ÅŸ. DB'yi sÄ±fÄ±rlayÄ±n: `docker compose down -v` ardÄ±ndan `docker compose up --build` |
| **LLM BaÄŸlantÄ± HatasÄ±** | 1. `.env` iÃ§indeki `LLM_BASE_URL` doÄŸru mu?<br>

<br>2. Ollama Ã§alÄ±ÅŸÄ±yor mu?<br>

<br>3. Model adÄ± doÄŸru mu? |
| **GPU Metrikleri Random Geliyor** | Container iÃ§inde NVIDIA sÃ¼rÃ¼cÃ¼leri yoktur. Bu proje, GPU eriÅŸimi yoksa test amaÃ§lÄ± rastgele veri Ã¼retir. |

---

> ğŸ“ **Not:** BazÄ± metrikler sistemden okunamadÄ±ÄŸÄ±nda bu proje **random** deÄŸerler yazar. Bu davranÄ±ÅŸ sadece Ã¶ÄŸrenme/deneme amaÃ§lÄ±dÄ±r. GerÃ§ek sistemlerde "unavailable" olarak iÅŸaretlenmesi Ã¶nerilir.

```

```
